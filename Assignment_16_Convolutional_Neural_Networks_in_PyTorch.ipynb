{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Lf-rRctoEHdX"
      ],
      "authorship_tag": "ABX9TyPfweXdzH+Z4KluIbgGDAcz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olinml2024/notebooks/blob/main/Assignment_16_Convolutional_Neural_Networks_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks in PyTorch\n",
        "\n",
        "In this notebook, we will introduce convolutional neural networks, learning how to train them and how to use them to make predictions. It includes some review illustration of convolutions and pooling.\n",
        "\n",
        "\n",
        "Note: In addition to Sam and Paul, this notebook builds on code from a [notebook](https://colab.research.google.com/github/goodboychan/chans_jupyter/blob/main/_notebooks/2020-07-29-01-Convolutional-Neural-Networks-in-PyTorch.ipynb#scrollTo=EPDNHRE5PXYI) from Google by Chanseok Kang and includes material from a lecture \"Introduction to Deep Learning with PyTorch\" via datacamp, Stanfords CS321, and ChatGPT which sources from multiple places."
      ],
      "metadata": {
        "id": "Dn8JnVeZBPi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises:\n",
        "* manipulate the variables until you can overfit the data.\n",
        "* read some parts of the code\n",
        "* write a simple neural network (maybe make them sort out the dimensions in class)\n",
        "* write your own model for desserts\n",
        "* use transfer learning\n",
        "\n",
        "add in some code to visualize the class output over time for a test image with no grad."
      ],
      "metadata": {
        "id": "TRI7A6TqlX6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVtJOuWZBJiu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.utils.data import Subset, Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Print the current device\n",
        "if device.type == 'cuda':\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the data\n",
        "\n",
        "This code helps you choose a subset of the data. We are going to start with a small bit of data."
      ],
      "metadata": {
        "id": "Xwfcvz7pBxvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete transform for both grayscale and RGB images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize if necessary\n",
        "    #transforms.Resize((224, 224)),  # Resize if necessary\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: transforms.Normalize(0.5, 0.5)(x) if x.size(0) == 1 else transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(x))\n",
        "])\n",
        "\n",
        "data_source = 'mnist'\n",
        "\n",
        "match data_source:\n",
        "  case 'mnist':\n",
        "    # Load from MNIST\n",
        "    train_set = torchvision.datasets.MNIST('mnist', train=True, transform=transform,download=True)\n",
        "    test_set = torchvision.datasets.MNIST('mnist', train=False, transform=transform)\n",
        "  case 'cifar10':\n",
        "    # Load from CIFAR-10\n",
        "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "  case 'desserts':\n",
        "    from os import mkdir, system, path\n",
        "    if path.exists('machine_learning_a2/train_data/images')==False:\n",
        "      print(\"downloading data\")\n",
        "      !git clone https://github.com/olinml2024/machine_learning_a2\n",
        "      %cd machine_learning_a2\n",
        "      from data_utils import download_dessert_test_data\n",
        "      download_dessert_test_data()\n",
        "      import gdown\n",
        "      gdown.download(id=\"1k_KaPCCO37HC8eqpd0D0JhKyGhqM8_KL\", output=\"train_data.zip\")\n",
        "      mkdir('train_data')\n",
        "      system('unzip -q test_data.zip -d train_data')\n",
        "      %cd ..\n",
        "    train_set = datasets.ImageFolder('machine_learning_a2/train_data/images', transform)\n",
        "    test_set = datasets.ImageFolder('machine_learning_a2/test_data/images', transform)\n",
        "\n",
        "def create_balanced_subset(dataset, selected_classes, samples_per_class, transform=None,batch_size=32):\n",
        "    \"\"\"\n",
        "    Creates a balanced subset of the dataset with specified classes and samples per class.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: The original dataset (e.g., MNIST, CIFAR-10).\n",
        "    - selected_classes: List of class indices to include in the subset.\n",
        "    - samples_per_class: Number of examples to sample per class.\n",
        "    - transform: Optional transformation to apply to the images.\n",
        "\n",
        "    Returns:\n",
        "    - DataLoader for the balanced subset.\n",
        "    \"\"\"\n",
        "    # Ensure the dataset uses the expected format\n",
        "    if not isinstance(dataset, Dataset):\n",
        "        raise ValueError(\"The provided dataset must be a PyTorch Dataset.\")\n",
        "\n",
        "    # Group indices by class\n",
        "    class_indices = {cls: [] for cls in selected_classes}\n",
        "\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        if label in selected_classes:\n",
        "            class_indices[label].append(idx)\n",
        "\n",
        "    # Sample from each class\n",
        "    sampled_indices = []\n",
        "    for cls in selected_classes:\n",
        "        if len(class_indices[cls]) < samples_per_class:\n",
        "            raise ValueError(f\"Not enough samples for class {cls}. Available: {len(class_indices[cls])}, Required: {samples_per_class}\")\n",
        "        sampled_indices.extend(random.sample(class_indices[cls], samples_per_class))\n",
        "\n",
        "    # Create a subset and DataLoader\n",
        "    balanced_subset = Subset(dataset, sampled_indices)\n",
        "    return DataLoader(balanced_subset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "\n",
        "\n",
        "train_set.classes\n"
      ],
      "metadata": {
        "id": "gGwEI0adNykH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify classes to include and number of samples per class\n",
        "selected_classes = [0, 1, 2]  # Example: 0 = 0, 1 = 1, 2 = 2 (digits)\n",
        "train_samples_per_class = 10  # Number of examples per class\n",
        "test_samples_per_class = 110\n",
        "batch_size = 10\n",
        "\n",
        "# Create a balanced subset DataLoader\n",
        "train_loader = create_balanced_subset(train_set, selected_classes, train_samples_per_class, batch_size=batch_size)\n",
        "test_loader = create_balanced_subset(train_set, selected_classes, test_samples_per_class, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Check the number of batches, note this drops the last if not a full batch, which could exclude some of your data if the number of datapoints isn't divisible by the batch_size\n",
        "print(f'Number of batches in the DataLoader: {len(train_loader)}')\n",
        "\n",
        "num_classes = len(selected_classes)\n",
        "print(num_classes)\n"
      ],
      "metadata": {
        "id": "HgL1hiqmekQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to display a batch of images with labels\n",
        "def show_sample_images(data_loader, num_images=10):\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = next(iter(data_loader))  # Get the first batch\n",
        "\n",
        "    # Set the number of images to display\n",
        "    num_images = min(num_images, images.size(0))  # Ensure we don't exceed the batch size\n",
        "\n",
        "    # Create a grid of images\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        image = images[i].permute(1, 2, 0).detach().cpu().numpy()  # Rearrange to (H, W, C)\n",
        "        image = (image - image.min()) / (image.max() - image.min())  # Normalize for visualization\n",
        "        axes[i].imshow(image)\n",
        "        axes[i].set_title(f'Label: {labels[i].item()}')  # Show label\n",
        "        axes[i].axis('off')  # Turn off axis\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Show a sample of 10 images with labels from the train_loader\n",
        "show_sample_images(train_loader, num_images=10)\n",
        "show_sample_images(test_loader, num_images=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "flZynEbnJtH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model\n"
      ],
      "metadata": {
        "id": "VXbTOVG4EEDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN architecture\n",
        "\n",
        "class FC_only(nn.Module):\n",
        "  def __init__(self,num_classes=2):\n",
        "    super(FC_only, self).__init__()\n",
        "    self.fc2 = nn.Linear(32*32, num_classes)\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "QQ4p5f5JEIQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the gradients and activations"
      ],
      "metadata": {
        "id": "Lf-rRctoEHdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize the gradient and input activations\n",
        "def visualize_gradients_and_activations(inputs, labels, model,showGradients=False,showActivations=False,showImage=True):\n",
        "  if showImage:\n",
        "    # Show a sample input image\n",
        "    input_image = inputs[0]\n",
        "    print(input_image.shape)\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    if input_image.ndim == 3:  # Shape (C, H, W)\n",
        "      input_image = input_image.permute(1, 2, 0).detach().cpu().numpy()  # Rearrange to (H, W, C)\n",
        "    elif input_image.ndim == 2:  # Shape (H, W)\n",
        "      input_image = input_image.detach().cpu().numpy()\n",
        "    plt.imshow(input_image)  #\n",
        "    plt.title(f'Input Image - Label: {labels[0].item()}\\nDimensions: {input_image.shape}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  # Register hooks to capture gradients and activations\n",
        "  gradients = []\n",
        "  activations = []\n",
        "\n",
        "  def hook_fn(module, grad_input, grad_output):\n",
        "      gradients.append(grad_output)\n",
        "\n",
        "  def forward_hook(module, input, output):\n",
        "      activations.append(output)\n",
        "\n",
        "  # Attach hooks to the model's layers\n",
        "  for layer in model.children():\n",
        "    #if showActivations:\n",
        "    layer.register_forward_hook(forward_hook)\n",
        "    #if showGradients:\n",
        "    layer.register_backward_hook(hook_fn)\n",
        "\n",
        "  # Forward pass\n",
        "  outputs = model(inputs)\n",
        "  loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "  # Backward pass\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  if showActivations:\n",
        "    # Visualizing the activations\n",
        "    fig, axes = plt.subplots(1, len(activations), figsize=(15, 5))\n",
        "    for i, activation in enumerate(activations):\n",
        "        activation_image = activation[0].detach().cpu().numpy()  # Get the first sample\n",
        "        # Check the number of dimensions\n",
        "        if activation_image.ndim == 3:  # Shape (C, H, W)\n",
        "            activation_image = activation_image.transpose(1, 2, 0)  # Rearrange to (H, W, C)\n",
        "            if activation_image.shape[2] > 3:  # If there are more than 3 channels, take only the first 3\n",
        "                activation_image = activation_image[..., :3]\n",
        "        elif activation_image.ndim == 4:  # Shape (N, C, H, W)\n",
        "            activation_image = activation_image[0].detach().cpu().numpy()  # Get the first sample\n",
        "            activation_image = activation_image.transpose(1, 2, 0)  # Rearrange to (H, W, C)\n",
        "            if activation_image.shape[2] > 3:  # If there are more than 3 channels, take only the first 3\n",
        "                activation_image = activation_image[..., :3]\n",
        "        elif activation_image.ndim == 1:  # Handle 1D case if necessary\n",
        "            activation_image = activation_image.reshape(-1, 1)\n",
        "\n",
        "        # Normalize for visualization\n",
        "        activation_image = (activation_image - activation_image.min()) / (activation_image.max() - activation_image.min())\n",
        "\n",
        "        axes[i].imshow(activation_image)\n",
        "        axes[i].set_title(f'Activation of Layer {i+1}\\nDimensions: {activation_image.shape}')\n",
        "        axes[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "  if showGradients:\n",
        "    # Visualizing the gradients\n",
        "    fig, axes = plt.subplots(1, len(gradients), figsize=(15, 5))\n",
        "    for i, grad in enumerate(gradients):\n",
        "        grad_image = grad[0].detach().cpu().numpy()  # Get the first sample\n",
        "        # Check the number of dimensions\n",
        "        if grad_image.ndim == 3:  # Shape (C, H, W)\n",
        "            grad_image = grad_image.transpose(1, 2, 0)  # Rearrange to (H, W, C)\n",
        "            if grad_image.shape[2] > 3:  # If there are more than 3 channels, take only the first 3\n",
        "                grad_image = grad_image[..., :3]\n",
        "        elif grad_image.ndim == 4:  # Shape (N, C, H, W)\n",
        "            grad_image = grad_image[0]  # Get the first sample without .detach() and .cpu() since it should be a tensor\n",
        "            grad_image = grad_image.transpose(1, 2, 0)  # Rearrange to (H, W, C)\n",
        "            if grad_image.shape[2] > 3:  # If there are more than 3 channels, take only the first 3\n",
        "                grad_image = grad_image[..., :3]\n",
        "        elif grad_image.ndim == 1:  # Handle 1D case if necessary\n",
        "            grad_image = grad_image.reshape(-1, 1)\n",
        "\n",
        "\n",
        "        # Normalize for visualization\n",
        "        grad_image = (grad_image - grad_image.min()) / (grad_image.max() - grad_image.min())\n",
        "\n",
        "        axes[i].imshow(grad_image)\n",
        "        axes[i].set_title(f'Gradient of Layer {i+1}\\nDimensions: {grad_image.shape}')\n",
        "        axes[i].axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Oy8x-NdPEcHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n"
      ],
      "metadata": {
        "id": "rmVTzjCUFW1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = FC_only(num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Lists to store training and test losses\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Loop through training data\n",
        "    #print(\"Epoch: \" + str(epoch + 1))\n",
        "    for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the same device\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    # Average training loss for the epoch\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Evaluation phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_test_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  #\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_test_loss += loss.item()\n",
        "\n",
        "    # Average test loss for the epoch\n",
        "    avg_test_loss = running_test_loss / len(test_loader)\n",
        "    test_losses.append(avg_test_loss)\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0: # Every x epochs, print the loss\n",
        "      # Print losses for the current epoch\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
        "\n",
        "\n",
        "    # Visualize every epoch\n",
        "    if (epoch + 1) % 100 == 0:#\n",
        "      #gray_value = 0.99  # Gray level (0.0 to 1.0)\n",
        "      #gray_input = torch.full((4, 1, 32, 32), gray_value)  # Shape: (batch, channels, height, width)\n",
        "      visualize_gradients_and_activations(inputs, labels, model,showGradients=True,showActivations=True,showImage=True)\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "-fAYtSJhFfa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(test_losses, label='Test Loss', color='orange')\n",
        "plt.title('Training and Test Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zssN15VpeEbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "# Function to calculate confusion matrix\n",
        "def calculate_confusion_matrix(model, data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the same device\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the predicted class index\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    return np.array(all_labels), np.array(all_preds)\n",
        "\n",
        "# Calculate confusion matrices for training and test data\n",
        "train_labels, train_preds = calculate_confusion_matrix(model, train_loader)\n",
        "test_labels, test_preds = calculate_confusion_matrix(model, test_loader)\n",
        "\n",
        "# Compute confusion matrices\n",
        "train_cm = confusion_matrix(train_labels, train_preds)\n",
        "test_cm = confusion_matrix(test_labels, test_preds)\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "# Define class names for the confusion matrix (change based on your dataset)\n",
        "# You will need to fix these manually or extract them from your data\n",
        "class_names = ['Class 0', 'Class 1']\n",
        "\n",
        "# Plot confusion matrices\n",
        "plot_confusion_matrix(train_cm, class_names, title='Training Confusion Matrix')\n",
        "plot_confusion_matrix(test_cm, class_names, title='Test Confusion Matrix')\n",
        "\n",
        "# Calculate accuracy\n",
        "train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "\n",
        "# Calculate F1 scores\n",
        "train_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "# Print accuracy and F1 scores\n",
        "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "print(f'Training F1 Score: {train_f1:.4f}')\n",
        "print(f'Test F1 Score: {test_f1:.4f}')"
      ],
      "metadata": {
        "id": "sbDWhQZuhZB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}