{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJHxRJG8KmPfeeBsWGiEeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olinml2024/notebooks/blob/main/ML24_Assignment11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on With Word2Vec\n",
        "\n",
        "In this notebook you are going to take word2vec embeddings for a spin.  These embeddings were introduced in the paper, and represent [a very influential paper in the field](https://arxiv.org/abs/1301.3781).  The field of natural language process (NLP) has certainly progressed far beyond word2vec, but it remains a useful example to learn about.\n",
        "\n",
        "We'll start out by loading a commonly used pre-trained set of word embeddings based on the Google Newsgroup Dataset."
      ],
      "metadata": {
        "id": "CogOFMSMNYsA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "EAxx0UrYNSoT",
        "outputId": "62b8f0de-3108-49c5-ff97-88d1cdd49c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
            "From (redirected): https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&confirm=t&uuid=d737beb6-68e2-442e-a046-55cb7613d446\n",
            "To: /content/GoogleNews-vectors-negative300.bin.gz\n",
            "100%|██████████| 1.65G/1.65G [00:26<00:00, 62.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GoogleNews-vectors-negative300.bin.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "gdown.download(id='0B7XkCwpI5KDYNlNUTTlSS21pQmM')\n",
        "!gunzip -k GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can parse the vectors and calculate words that are most similar to a test word."
      ],
      "metadata": {
        "id": "hCJ6HNVOQK9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load model from local path\n",
        "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "545HLRvbO2yj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('cat')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRd_euhDO6RW",
        "outputId": "d3547b48-a082-4c6e-d102-5eef0018c25e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cats', 0.8099379539489746),\n",
              " ('dog', 0.760945737361908),\n",
              " ('kitten', 0.7464985251426697),\n",
              " ('feline', 0.7326234579086304),\n",
              " ('beagle', 0.7150582671165466),\n",
              " ('puppy', 0.7075453400611877),\n",
              " ('pup', 0.6934291124343872),\n",
              " ('pet', 0.6891531348228455),\n",
              " ('felines', 0.6755931973457336),\n",
              " ('chihuahua', 0.6709762215614319)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's also possible to explore how the word embeddings themselves behave as a vector space (e.g., what happens when you add or subtract embeddings from each other).  Below is an example of the famous analogy from the original paper."
      ],
      "metadata": {
        "id": "O4AWjQ0GSwb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEDrGESeTB5Z",
        "outputId": "0ba91091-39ed-485c-9899-a899bb3ccb6a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118193507194519),\n",
              " ('monarch', 0.6189674139022827),\n",
              " ('princess', 0.5902431011199951),\n",
              " ('crown_prince', 0.5499460697174072),\n",
              " ('prince', 0.5377321839332581),\n",
              " ('kings', 0.5236844420433044),\n",
              " ('Queen_Consort', 0.5235945582389832),\n",
              " ('queens', 0.5181134343147278),\n",
              " ('sultan', 0.5098593831062317),\n",
              " ('monarchy', 0.5087411999702454)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also look for outliers using the ``doesnt_match`` function."
      ],
      "metadata": {
        "id": "nFK7UJ0_VHkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.doesnt_match(['orange', 'banana', 'computer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QL3-tlDFVLgT",
        "outputId": "26c11e7b-e09a-46f4-b96c-08b6036a1bf5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'computer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Exercise 1\n",
        "\n",
        "Play around with ``model.most_similar`` or ``doesnt_match``.  Summarize your observations.\n",
        "\n",
        "*Warning:* it's possible you may uncover some disturbing associations here.  We want you to share what you found, and we want you to be prepared that some of what you find may be shocking.  The word2vec model is known to contain gender and racial bias, which you'll read about later in this assignment."
      ],
      "metadata": {
        "id": "Cf7Cnm-ZQdf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access the word embeddings directly using the following syntax."
      ],
      "metadata": {
        "id": "vzrd7EyqRE2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"word embedding shape is {model['boy'].shape}\")\n",
        "print(model['boy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qGfNUJIQA2x",
        "outputId": "d02f6e5a-c3a2-4a35-f094-de7d6b04b55b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word embedding shape is (300,)\n",
            "[ 2.35351562e-01  1.65039062e-01  9.32617188e-02 -1.28906250e-01\n",
            "  1.59912109e-02  3.61328125e-02 -1.16699219e-01 -7.32421875e-02\n",
            "  1.38671875e-01  1.15356445e-02  1.87500000e-01 -2.91015625e-01\n",
            "  1.70898438e-02 -1.84570312e-01 -2.87109375e-01  2.54821777e-03\n",
            " -2.19726562e-01  1.77734375e-01 -1.20605469e-01  5.39550781e-02\n",
            "  3.78417969e-02  2.49023438e-01  1.76757812e-01  2.69775391e-02\n",
            "  1.21093750e-01 -3.51562500e-01 -5.83496094e-02  1.22070312e-01\n",
            "  5.97656250e-01 -1.60156250e-01  1.08398438e-01 -2.40478516e-02\n",
            " -1.16699219e-01  3.58886719e-02 -2.37304688e-01  1.15234375e-01\n",
            "  5.27343750e-01 -2.18750000e-01 -4.54101562e-02  3.30078125e-01\n",
            "  3.75976562e-02 -5.51757812e-02  3.26171875e-01  6.74438477e-03\n",
            "  3.71093750e-01  3.68652344e-02  6.68945312e-02  5.17578125e-02\n",
            " -4.76074219e-02 -7.91015625e-02  4.46777344e-02  1.67968750e-01\n",
            "  5.51757812e-02 -2.91015625e-01  2.59765625e-01 -1.00097656e-01\n",
            " -1.09863281e-01 -9.15527344e-03  2.63671875e-02 -3.44238281e-02\n",
            "  9.37500000e-02  3.53515625e-01  8.39843750e-02 -7.75146484e-03\n",
            "  8.64257812e-02 -5.24902344e-02 -5.59082031e-02 -8.59375000e-02\n",
            "  5.37109375e-02 -1.47094727e-02  3.63769531e-02  4.68750000e-02\n",
            " -3.39843750e-01  1.28906250e-01 -1.22558594e-01  4.57031250e-01\n",
            "  1.27929688e-01 -2.89062500e-01  1.56250000e-01  3.73535156e-02\n",
            "  2.75390625e-01 -1.28784180e-02 -1.50390625e-01 -1.64062500e-01\n",
            " -3.39843750e-01  8.00781250e-02 -9.21630859e-03  2.78320312e-02\n",
            "  9.32617188e-02  2.25830078e-02 -1.62353516e-02 -8.25195312e-02\n",
            " -1.90429688e-02 -3.49121094e-02  9.42382812e-02  3.66210938e-02\n",
            "  6.39648438e-02  2.00195312e-01 -4.05273438e-02 -1.08886719e-01\n",
            " -3.93676758e-03 -2.55859375e-01  6.78710938e-02 -1.89453125e-01\n",
            "  1.72851562e-01 -1.73828125e-01  2.07031250e-01 -1.59179688e-01\n",
            "  2.85339355e-03 -1.80664062e-01 -6.93359375e-02  2.05078125e-01\n",
            "  5.93261719e-02 -2.17773438e-01 -1.36718750e-01 -4.91333008e-03\n",
            " -1.38671875e-01 -7.47070312e-02 -3.54003906e-02  1.13769531e-01\n",
            "  3.07617188e-02 -1.05957031e-01 -3.30078125e-01 -2.72216797e-02\n",
            " -1.94091797e-02  9.52148438e-02  8.69140625e-02 -2.16064453e-02\n",
            " -6.98242188e-02 -1.73828125e-01 -1.60156250e-01 -2.44140625e-01\n",
            "  9.82666016e-03  2.24609375e-02 -2.13867188e-01  1.91406250e-01\n",
            "  2.01171875e-01  2.72216797e-02  2.81982422e-02  2.42187500e-01\n",
            "  3.55468750e-01 -5.32226562e-02  1.78710938e-01  6.78710938e-02\n",
            " -6.73828125e-02  3.49609375e-01 -1.92382812e-01 -1.00097656e-02\n",
            " -2.05078125e-01 -1.59179688e-01  3.76953125e-01 -2.15820312e-01\n",
            " -2.36328125e-01  6.49414062e-02 -1.39770508e-02  4.22363281e-02\n",
            "  2.51464844e-02 -1.00585938e-01  1.37695312e-01 -2.43164062e-01\n",
            "  1.20605469e-01  2.03857422e-02  3.12500000e-01  1.09863281e-01\n",
            " -1.04980469e-01 -9.13085938e-02  2.21679688e-01 -1.04003906e-01\n",
            "  1.25976562e-01  5.10253906e-02  6.39648438e-02 -1.15722656e-01\n",
            " -3.19824219e-02 -8.34960938e-02 -1.97265625e-01 -2.33154297e-02\n",
            "  1.94335938e-01  2.24609375e-01 -2.30468750e-01  4.17480469e-02\n",
            "  6.49414062e-02 -1.70898438e-01  7.86132812e-02 -3.58886719e-02\n",
            " -1.66015625e-01  2.25585938e-01  1.23535156e-01  1.08398438e-01\n",
            "  1.15722656e-01  7.37304688e-02 -1.56250000e-02 -5.85937500e-02\n",
            " -8.93554688e-02  1.30859375e-01  1.90429688e-01 -3.58886719e-02\n",
            " -1.36718750e-02 -1.88476562e-01 -1.48437500e-01 -2.51953125e-01\n",
            " -1.22558594e-01 -2.75390625e-01 -1.54296875e-01 -2.83203125e-01\n",
            "  1.10839844e-01 -2.46093750e-01  1.89453125e-01 -2.50244141e-02\n",
            "  8.59375000e-02 -1.17675781e-01 -2.46582031e-02 -1.32812500e-01\n",
            "  1.00097656e-01 -2.45117188e-01 -2.02148438e-01 -7.56835938e-02\n",
            "  6.03027344e-02  1.72851562e-01 -6.59179688e-02  6.78710938e-02\n",
            "  6.98242188e-02 -4.10156250e-02  2.14843750e-01  7.17773438e-02\n",
            " -4.57763672e-03 -4.04357910e-04  8.59375000e-02 -2.55859375e-01\n",
            " -4.32128906e-02 -1.31835938e-01  2.05078125e-02 -2.46093750e-01\n",
            " -1.28906250e-01  1.23535156e-01 -1.48437500e-01  5.15136719e-02\n",
            " -1.55273438e-01 -1.70898438e-01  1.92382812e-01  2.16796875e-01\n",
            "  5.81054688e-02 -1.28906250e-01 -1.43554688e-01 -7.78198242e-03\n",
            " -1.15234375e-01  4.08203125e-01 -3.37890625e-01  8.64257812e-02\n",
            "  2.08007812e-01  2.35595703e-02  1.36718750e-01 -4.71191406e-02\n",
            "  9.91210938e-02  1.18164062e-01  1.19140625e-01  1.24511719e-01\n",
            "  4.66308594e-02  5.41992188e-02 -2.11914062e-01 -8.20312500e-02\n",
            " -5.17578125e-02  2.03857422e-02 -1.59179688e-01 -1.76757812e-01\n",
            "  8.54492188e-02  1.38671875e-01 -1.01562500e-01  2.61230469e-02\n",
            " -1.88476562e-01 -1.57470703e-02  1.21093750e-01 -9.66796875e-02\n",
            "  2.13623047e-02 -6.68945312e-02 -2.69775391e-02  3.51562500e-02\n",
            "  1.68945312e-01  1.55639648e-02 -1.25976562e-01 -1.44531250e-01\n",
            "  1.78710938e-01 -7.42187500e-02  2.72216797e-02  4.98046875e-01\n",
            " -6.03027344e-02 -1.35742188e-01 -1.62109375e-01  9.57031250e-02\n",
            " -1.84326172e-02  3.90625000e-01  1.90429688e-02 -1.03149414e-02\n",
            " -1.15234375e-01 -2.91015625e-01 -5.95703125e-02 -5.37109375e-02\n",
            " -7.42187500e-02 -2.65625000e-01 -1.03027344e-01  1.35742188e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  We leave it up to you to see if you want to explore these embeddings further.  They can be used, e.g., as a way to encode word to solve a supervised learning task (e.g., sentiment classification).  If you're interested in combining these two ideas, you can check out [bag of words meets bags of popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial) on the Kaggle website."
      ],
      "metadata": {
        "id": "6W1RhyGNUQnI"
      }
    }
  ]
}