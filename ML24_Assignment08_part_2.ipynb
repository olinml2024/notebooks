{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "5DzGXfkg9Ugp",
        "7WLOaBHmJJsI"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olinml2024/notebooks/blob/main/ML24_Assignment08_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OklypbSOJI4V"
      },
      "source": [
        "# Assignment 8 Companion Notebook (part 2): Neural Network Implementation in Pytorch\n",
        "\n",
        "Learning Objectives:\n",
        "* Implement a multi-layer perceptron in pytorch\n",
        "* Create learning curves to understand model training\n",
        "* Learn about batch-based training\n",
        "\n",
        "## Recognizing Digits\n",
        "\n",
        "Recognizing handwritten digits using machine learning is embedded so thoroughly into the discipline's early history, that anytime someone in the field brings it up at a conference you are likely to elicit groans from the audience.  Feel free to groan at us whenever we mention it in class.\n",
        "\n",
        "We're going to use a digit recognition dataset that is built into sklearn.  It's not the famous MNIST dataset, but it's the same idea.  Once we've loaded the digits, we'll create some models using pytorch to predict the identity of a digit from its pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vFl9kn1Iyx4"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "print(digits.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some sample images from the dataset."
      ],
      "metadata": {
        "id": "Hq6WPnMrRP8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "fig, axs = plt.subplots(4, 2)\n",
        "axs = axs.flatten()             # we flatten the axes to make indexing easier\n",
        "for i in range(8):\n",
        "    axs[i].imshow(digits['data'][i,:].reshape((8,8)), cmap='gray')\n",
        "    axs[i].axes.xaxis.set_visible(False)\n",
        "    axs[i].axes.yaxis.set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AZBi2E5xQEPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the part we will look at in class.\n",
        "\n"
      ],
      "metadata": {
        "id": "hGZnneanRbHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    \"\"\" A model that implements a logistic regression classifier. \"\"\"\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        # initialize the model weights\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Implement the forward pass of the model. \"\"\"\n",
        "        out = self.linear(x)\n",
        "        # We leave of the softmax here as we are going to use cross entropy loss\n",
        "        # which applies the softmax for us.\n",
        "        # out = F.softmax(out, dim=1)\n",
        "        return out\n",
        "\n",
        "# see if we can run on the GPU (change your runtime to T4 if you want cuda)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# each input is 64 features (8x8 pixels) and there are 10 possible digits\n",
        "model = LogisticRegression(64, 10).to(device)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits['data'],\n",
        "                                                    digits['target'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# we need to convert from numpy to pytorch and also move teh data to the GPU\n",
        "# (if we are running on a machine with a GPU)\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32)).to(device)\n",
        "y_train = torch.from_numpy(y_train).to(device)\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32)).to(device)\n",
        "y_test = torch.from_numpy(y_test).to(device)\n",
        "\n",
        "# you might need to adjust this if you are not using a GPU instance\n",
        "n_epochs = 1000\n",
        "learning_rate = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "train_losses = np.zeros((n_epochs,))\n",
        "test_losses = np.zeros((n_epochs,))\n",
        "accuracies = np.zeros((n_epochs,))\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    train_losses[epoch] = loss.item()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(X_test)\n",
        "        test_loss = criterion(test_outputs, y_test)\n",
        "        test_losses[epoch] = test_loss.item()\n",
        "        accuracies[epoch] = (torch.sum(test_outputs.argmax(dim=1) == y_test) / y_test.shape[0]).item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(n_epochs), train_losses, label='training cross-entropy')\n",
        "plt.plot(range(n_epochs), test_losses, label='testing cross-entropy')\n",
        "plt.xlabel('step')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(n_epochs), accuracies)\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('accuarcy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dtHYqM29RhEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2 Exercise 1\n",
        "\n",
        "Using the code above as a starting point, write code to train an MLP on the same dataset.  Your MLP should allow you to specify the number of hidden units to use.  Create a plot that compares the training and test cross entropies for both the logistic regression model and the MLP.  Create a similar plot that shows the same for accuracy.\n",
        "\n",
        "*Bonus: experiment with different numbers of hidden units and tell us what you find in terms of changes in accuracy or learning time*"
      ],
      "metadata": {
        "id": "C_4nMMPxWLrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "5DzGXfkg9Ugp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, num_classes):\n",
        "#         super(MLP, self).__init__()\n",
        "#         self.linear_1 = nn.Linear(input_size, hidden_size)\n",
        "#         self.linear_2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.linear_1(x)\n",
        "#         out = F.sigmoid(out)\n",
        "#         out = self.linear_2(out)\n",
        "#         # again: leave this out since our loss function already incorporates it\n",
        "#         # out = F.softmax(out, dim=1)\n",
        "#         return out\n",
        "\n",
        "# model = MLP(64, 50, 10).to(device)\n",
        "\n",
        "# n_epochs_mlp = 2000\n",
        "# learning_rate = 0.1\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# train_losses_mlp = np.zeros((n_epochs_mlp,))\n",
        "# test_losses_mlp = np.zeros((n_epochs_mlp,))\n",
        "# accuracies_mlp = np.zeros((n_epochs_mlp,))\n",
        "\n",
        "# for epoch in range(n_epochs_mlp):\n",
        "#     optimizer.zero_grad()\n",
        "#     outputs = model(X_train)\n",
        "#     loss = criterion(outputs, y_train)\n",
        "#     train_losses_mlp[epoch] = loss.item()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_outputs = model(X_test)\n",
        "#         test_loss = criterion(test_outputs, y_test)\n",
        "#         test_losses_mlp[epoch] = test_loss.item()\n",
        "#         accuracies_mlp[epoch] = (torch.sum(test_outputs.argmax(dim=1) == y_test) / y_test.shape[0]).item()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(range(n_epochs_mlp), train_losses_mlp, label='train cross-entropy (mlp)')\n",
        "# plt.plot(range(n_epochs_mlp), test_losses_mlp, label='test cross-entropy (mlp)')\n",
        "# plt.plot(range(n_epochs), train_losses, label='train cross-entropy (logistic)')\n",
        "# plt.plot(range(n_epochs), test_losses, label='test cross-entropy (logistic)')\n",
        "# plt.xlabel('step')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(range(n_epochs_mlp), accuracies_mlp, label='accuracy (mlp)')\n",
        "# plt.plot(range(n_epochs), accuracies, label='accuracy (logistic)')\n",
        "# plt.xlabel('step')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# print(f\"final MLP accuracy: {accuracies_mlp[-1]}\")\n",
        "# print(f\"final MLP training loss: {train_losses_mlp[-1]}\")"
      ],
      "metadata": {
        "id": "x-B4lix9Rrro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing Using Batches\n",
        "\n",
        "So far we've been using gradient descent where we take the gradient of the our loss function with computed over *all* of our training data.  There are many situations where this will not work (e.g., if you have a really big dataset, you may not be able to fit all of your data in the computer's memory at one time).\n",
        "\n",
        "An alternative to training where we process data all at once is to process data in batches.  A batch of data is simply a randomly chosen subset of the training data (e.g., we might choose a set of a 100 random images of digits).  In the next exercise you'll be modifying your code to use batch-based."
      ],
      "metadata": {
        "id": "lZQSkQAJ9mF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2 Exercise 2\n",
        "\n",
        "Modify the code from Exercise 1 to train using batches.  Your code should take an optimization step for each batch of data.  There are plenty of resources online to help with this, but here is some code that might be helpful (see next cell).  This code wraps the training data in a dataset object and then creates a data loader that will allow us to iterate over batches of data.\n",
        "\n",
        "Your code should create a plot or some sort of output that helps you understand how batch-based training compares to training on the whole dataset each step.\n",
        "\n",
        "Make some notes about what you perceive to be different between the two types of training."
      ],
      "metadata": {
        "id": "jbAy5QsmIG3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset,\n",
        "                                          batch_size=50,\n",
        "                                          shuffle=True)"
      ],
      "metadata": {
        "id": "U4UbCEdhIvCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "7WLOaBHmJJsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using batches for training seems to achieve high accuracy much faster.  The training and test losses continue to decline after the accuracy saturates.\n",
        " At least for this problem, there doesn't seem to be much of a downside of this approach versus using the whole dataset for each gradient descent step."
      ],
      "metadata": {
        "id": "TEY2Y3zIJMKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, num_classes):\n",
        "#         super(MLP, self).__init__()\n",
        "#         self.linear_1 = nn.Linear(input_size, hidden_size)\n",
        "#         self.linear_2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.linear_1(x)\n",
        "#         out = F.sigmoid(out)\n",
        "#         out = self.linear_2(out)\n",
        "#         # leave this out since CrossEntropyLoss takes care of it for us\n",
        "#         # out = F.softmax(out, dim=1)\n",
        "#         return out\n",
        "\n",
        "# model = MLP(64, 50, 10).to(device)\n",
        "\n",
        "# n_epochs_mlp = 100\n",
        "# learning_rate = 0.05\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# train_losses_mlp = np.zeros((n_epochs_mlp,))\n",
        "# test_losses_mlp = np.zeros((n_epochs_mlp,))\n",
        "# accuracies_mlp = np.zeros((n_epochs_mlp,))\n",
        "\n",
        "# trainset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset,\n",
        "#                                           batch_size=50,\n",
        "#                                           shuffle=True)\n",
        "# for epoch in range(n_epochs_mlp):\n",
        "#     with torch.no_grad():\n",
        "#         test_outputs = model(X_test)\n",
        "#         test_loss = criterion(test_outputs, y_test)\n",
        "#         test_losses_mlp[epoch] = test_loss.item()\n",
        "#         accuracies_mlp[epoch] = (torch.sum(test_outputs.argmax(dim=1) == y_test) / y_test.shape[0]).item()\n",
        "\n",
        "#         train_outputs = model(X_train)\n",
        "#         train_loss = criterion(train_outputs, y_train)\n",
        "#         train_losses_mlp[epoch] = train_loss.item()\n",
        "\n",
        "#     for i, (inputs, labels) in enumerate(trainloader):\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(range(n_epochs_mlp), train_losses_mlp, label='train cross-entropy (mlp)')\n",
        "# plt.plot(range(n_epochs_mlp), test_losses_mlp, label='test cross-entropy (mlp)')\n",
        "# plt.plot(range(n_epochs), train_losses, label='train cross-entropy (logistic)')\n",
        "# plt.plot(range(n_epochs), test_losses, label='test cross-entropy (logistic)')\n",
        "# plt.xlabel('step')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(range(n_epochs_mlp), accuracies_mlp, label='accuracy (mlp)')\n",
        "# plt.plot(range(n_epochs), accuracies, label='accuracy (logistic)')\n",
        "# plt.xlabel('step')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# print(f\"final MLP accuracy: {accuracies_mlp[-1]}\")\n",
        "# print(f\"final MLP training loss: {train_losses_mlp[-1]}\")"
      ],
      "metadata": {
        "id": "FTI8ur7d9rn6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}