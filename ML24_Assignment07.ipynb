{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "HRqgZ567QHQH",
        "WDHma3NI5M-h",
        "xHdBuQtBTVOI",
        "WYQccq6_EXSx",
        "He0_h3cjHXvh",
        "PLSGHVVGcSGe",
        "yAhPtGAOKuKd",
        "BF8fNwZ4UhZ7"
      ],
      "authorship_tag": "ABX9TyOdyKprOx1NPE9+JZUQfwGY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olinml2024/notebooks/blob/main/ML24_Assignment07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 7: Autograd\n",
        "\n",
        "**High-level picture:** In this assignment you'll be implementing your very own system for automatic differentiation.  You'll then see how this framework can be used as a machine learning algorithm by using your autograd engine within a gradient descent optimizer.\n",
        "\n",
        "*Credits:* the design of the autograd engine you will be implementing is based on the micrograd framework from **Andrej Karpathy**.  You may find it useful [to watch Andrej Karpathy's walkthrough video](https://www.youtube.com/watch?v=VMj-3S1tku0&t=2414s) of this.  A lot of what we've done in the first 2/3 of the assignment follows along the same lines, but we felt it important to explicitly define exercises so that you are actively doing things and reasoning about what you are implementing rather than passively watching.\n",
        "\n",
        "Before we start, here is some possibly useful background for those with little familiarity with the idea of neural networks and gradient descent (we'll go into this soon, but we thought some might find these videos useful now).\n",
        "* [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk)\n",
        "* [Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w)"
      ],
      "metadata": {
        "id": "W4XJxHcvIzCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "Let's do a little calculus in Python.\n",
        "\n",
        "Define a function, `f`, that takes a scalar, `x`, as input and returns the expression $x^2 + 5x$.  Define a second function, `df`, that takes a scalar, `x`, as input and returns $\\frac{d}{dx}f(x)$ (i.e., the derivative of $f$).  Verify that your function returns the correct derivative by computing a numerical approximation of the derivative.  **Hint:** to numerically estimate the derivative, calculate the ratio of rise over run as you nudge the input to the function, `f`, by a small amount.  $f'(x) \\approx \\frac{f(x+h) - f(x)}{h}$ (where $h$ is some small value, like $0.001$).\n",
        "\n",
        "Define a function, `g`, that takes two scalars as input, `x` and `y`, and returns the expression $sin(x)y^2$.  Define two additional functions that return $\\frac{\\partial g}{\\partial x}$ and $\\frac{\\partial g}{\\partial y}$.  As before, verify that your partial derivative functions are working by taking a numerical approximation."
      ],
      "metadata": {
        "id": "AGvPZC7e3Deb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution\n"
      ],
      "metadata": {
        "id": "HRqgZ567QHQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from math import sin, cos\n",
        "\n",
        "# def f(x):\n",
        "#     return x**2 + 5*x\n",
        "\n",
        "# def df(x):\n",
        "#     # doin' the real calculus\n",
        "#     return 2*x + 5\n",
        "\n",
        "# x = 3\n",
        "# h = 0.001\n",
        "\n",
        "# df_approx = (f(x+h) - f(x))/h\n",
        "\n",
        "# print(f\"derivative using calculus {df(x)} approximation of derivative {df_approx}\")\n",
        "\n",
        "# def g(x,y):\n",
        "#     return sin(x)*y**2\n",
        "\n",
        "# def dg_dx(x,y):\n",
        "#     return cos(x)*y**2\n",
        "\n",
        "# def dg_dy(x,y):\n",
        "#     return 2*y*sin(x)\n",
        "\n",
        "# x = 2\n",
        "# y = 1\n",
        "\n",
        "# dg_dx_approx = (g(x+h,y) - g(x,y))/h\n",
        "# dg_dy_approx = (g(x,y+h) - g(x,y))/h\n",
        "\n",
        "# print(f\"derivative using calculus {dg_dx(x,y)} approximation of derivative {dg_dx_approx}\")\n",
        "# print(f\"derivative using calculus {dg_dy(x,y)} approximation of derivative {dg_dy_approx}\")"
      ],
      "metadata": {
        "id": "7-si4gdfI3GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Let's create a Python class called `Value` that can store a scalar value. Define an addition function that allows you to add two Value objects together.  You'll also keep a placeholder to store, as a Python `set`, the `Value` objects (if any) that are used to compute the scalar stored in the `Value` object (e.g., if a `Value` object is computed as the sum of two other `Value` objects, you'd want to keep track of that in this set).  This might be a bit confusing at first, so we'll give you some commented starter code.  Once you've defined your addition function, create a suitable test case (nothing formal just a few print statements) to make sure it's working."
      ],
      "metadata": {
        "id": "Z4nzhllk42oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Value:\n",
        "    def __init__(self, data, inputs=()):\n",
        "        self.data = data\n",
        "        self.inputs = set(inputs)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        \"\"\" return a new `Value` object that represents the sum of `self` and `other` \"\"\"\n",
        "        pass\n",
        "\n",
        "# write a test case here."
      ],
      "metadata": {
        "id": "YgUO3lRO59FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "WDHma3NI5M-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Value:\n",
        "#     def __init__(self, data, inputs=()):\n",
        "#         self.data = data\n",
        "#         self.inputs = set(inputs)\n",
        "\n",
        "#     def __add__(self, other):\n",
        "#         return Value(self.data + other.data, (self, other))\n",
        "\n",
        "# x = Value(3)\n",
        "# y = Value(7)\n",
        "# z = x + y\n",
        "# print(z.data)       # should be 10"
      ],
      "metadata": {
        "id": "P0dRt6MYPY1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "Add a multiplication function to your `Value` class.  You can copy-paste your code from exercise 2 into the cell below so you  have a record of how your code was for exercise 2.  In order to use Python's syntax `a*b`, you'll want to call your multiplication function `__mul__`.\n",
        "\n",
        "Create a suitable test case to make sure your new function is working properly."
      ],
      "metadata": {
        "id": "v4bswJi85Pwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "xHdBuQtBTVOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Value:\n",
        "#     def __init__(self, data, inputs=()):\n",
        "#         self.data = data\n",
        "#         self.inputs = set(inputs)\n",
        "\n",
        "#     def __add__(self, other):\n",
        "#         return Value(self.data + other.data, (self, other))\n",
        "\n",
        "#     def __mul__(self, other):\n",
        "#         return Value(self.data * other.data, (self, other))\n",
        "\n",
        "# q = Value(2) * Value(3)\n",
        "# print(q.data)           # should be 6"
      ],
      "metadata": {
        "id": "T3PMdGI1QxAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4\n",
        "\n",
        "Draw a dataflow diagram representing the expression $f = xy + yz$.  Work out the partial derivatives using the methods we learned about in assignment 5."
      ],
      "metadata": {
        "id": "6Ssi00S7TZZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "JwkmJzP87RfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/olinml2024/notebooks/refs/heads/main/images/a7_dataflow_v2.png\" width=\"50%\"/>\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial f}{\\partial x} &= \\frac{\\partial r}{\\partial x} \\frac{\\partial f}{\\partial r} \\\\\n",
        "&= y \\\\\n",
        "\\frac{\\partial f}{\\partial y} &= \\frac{\\partial r}{\\partial y} \\frac{\\partial f}{\\partial r} + \\frac{\\partial s}{\\partial y} \\frac{\\partial f}{\\partial s} \\\\\n",
        "&= x + z \\\\\n",
        "\\frac{\\partial f}{\\partial y} &= \\frac{\\partial s}{\\partial z} \\frac{\\partial f}{\\partial s} \\\\\n",
        "&= y\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "Xrxq37xXdxu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5\n",
        "\n",
        "In the code block below, we've provided you with some code to visualize dataflow diagrams created with your `Value` class.  \n",
        "First, modify your `Value` class so it stores the name of the operation used to compute `Value` (e.g., + or *).  You should store this as a atttribute of your `Value` class called `op`.  Next, use your value class to generate the expression from exercise 4 and use the `draw_dot` function to visualize it.  Feel free to set the values of `x`, `y`, and `z` to whatever you want.  Make sure that the visualization of the dataflow diagram looks right."
      ],
      "metadata": {
        "id": "Cql2GbbATgcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "    nodes, edges = set(), set()\n",
        "    def build(v):\n",
        "        if v not in nodes:\n",
        "            nodes.add(v)\n",
        "            for child in v.inputs:\n",
        "                edges.add((child, v))\n",
        "                build(child)\n",
        "    build(root)\n",
        "    return nodes, edges\n",
        "\n",
        "def draw_dot(root, format='svg', rankdir='LR'):\n",
        "    \"\"\"\n",
        "    format: png | svg | ...\n",
        "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
        "    \"\"\"\n",
        "    assert rankdir in ['LR', 'TB']\n",
        "    nodes, edges = trace(root)\n",
        "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
        "\n",
        "    for n in nodes:\n",
        "        dot.node(name=str(id(n)), label = \"{ data %.4f }\" % (n.data), shape='record')\n",
        "        if n.op:\n",
        "            dot.node(name=str(id(n)) + n.op, label=n.op)\n",
        "            dot.edge(str(id(n)) + n.op, str(id(n)))\n",
        "\n",
        "    for n1, n2 in edges:\n",
        "        dot.edge(str(id(n1)), str(id(n2)) + n2.op)\n",
        "\n",
        "    return dot"
      ],
      "metadata": {
        "id": "nf4L8pVQRD5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "WYQccq6_EXSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from graphviz import Digraph\n",
        "\n",
        "# def trace(root):\n",
        "#     nodes, edges = set(), set()\n",
        "#     def build(v):\n",
        "#         if v not in nodes:\n",
        "#             nodes.add(v)\n",
        "#             for child in v.inputs:\n",
        "#                 edges.add((child, v))\n",
        "#                 build(child)\n",
        "#     build(root)\n",
        "#     return nodes, edges\n",
        "\n",
        "# def draw_dot(root, format='svg', rankdir='LR'):\n",
        "#     \"\"\"\n",
        "#     format: png | svg | ...\n",
        "#     rankdir: TB (top to bottom graph) | LR (left to right)\n",
        "#     \"\"\"\n",
        "#     assert rankdir in ['LR', 'TB']\n",
        "#     nodes, edges = trace(root)\n",
        "#     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
        "\n",
        "#     for n in nodes:\n",
        "#         dot.node(name=str(id(n)), label = \"{ data %.4f }\" % (n.data), shape='record')\n",
        "#         if n.op:\n",
        "#             dot.node(name=str(id(n)) + n.op, label=n.op)\n",
        "#             dot.edge(str(id(n)) + n.op, str(id(n)))\n",
        "\n",
        "#     for n1, n2 in edges:\n",
        "#         dot.edge(str(id(n1)), str(id(n2)) + n2.op)\n",
        "\n",
        "#     return dot\n",
        "\n",
        "# class Value:\n",
        "#     def __init__(self, data, inputs=(), op=\"\"):\n",
        "#         self.data = data\n",
        "#         self.inputs = set(inputs)\n",
        "#         self.op = op\n",
        "\n",
        "#     def __add__(self, other):\n",
        "#         return Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "#     def __mul__(self, other):\n",
        "#         return Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "# x = Value(7)\n",
        "# y = Value(4)\n",
        "# z = Value(2)\n",
        "# q = x*y + z*z*x\n",
        "# draw_dot(q)"
      ],
      "metadata": {
        "id": "tIWHgisTEY-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6\n",
        "\n",
        "Let's add a placeholder to store the partial derivative of some output expression with respect to the `data` stored in a `Value` object (let's call this partial derivative `grad` and store it as an attribute of your `Value` class).  In the `__init__` method of your class we'll set its initial value to 0.  We'll also update our visualization code to display `grad`.\n",
        "\n",
        "We'll also define a function called `_backward` that will propagate the appropriate partial derivatives to each of `Value`'s inputs (take a look at the figure below to see the picture from day 6).\n",
        "\n",
        "<img src=\"https://mermaid.ink/svg/eyJjb2RlIjoiZmxvd2NoYXJ0IFRCXG5pZDFbXCIkJGdyYWRfZiA9IDEgfn5-fiQkXCJdXG5pZDJbXCIkJGdyYWRfeCA9IFxcZnJhY3tcXHBhcnRpYWwgZn17XFxwYXJ0aWFsIHh9IGdyYWRfZn5-JCRcIl1cbmlkM1tcIiQkZ3JhZF95ID0gXFxmcmFje1xccGFydGlhbCBmfXtcXHBhcnRpYWwgeX0gZ3JhZF9mfn4kJFwiXVxuaWQ0W1wiJCRncmFkX3QgPSBcXGZyYWN7XFxwYXJ0aWFsIHh9e1xccGFydGlhbCB0fSBncmFkX3ggKyBcXGZyYWN7XFxwYXJ0aWFsIHl9e1xccGFydGlhbCB0fSBncmFkX3l-fn5-fn4kJFwiXVxuaWQxIC0tXCIkJFxcZnJhY3tcXHBhcnRpYWwgZn17XFxwYXJ0aWFsIHh9IGdyYWRfZn5-JCRcIi0tPiBpZDJcbmlkMSAtLVwiJCRcXGZyYWN7XFxwYXJ0aWFsIGZ9e1xccGFydGlhbCB5fSBncmFkX2Z-fiQkXCItLT4gaWQzXG5pZDIgLS1cIiQkXFxmcmFje1xccGFydGlhbCB4fXtcXHBhcnRpYWwgdH0gZ3JhZF94IH5-JCRcIi0tPiBpZDRcbmlkMyAtLVwiJCRcXGZyYWN7XFxwYXJ0aWFsIHl9e1xccGFydGlhbCB0fSBncmFkX3kgfn4kJFwiLS0-IGlkNCIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In19\" width=\"50%\"/>\n",
        "\n",
        "This is probably a bit confusing, but we'll give you a template for the `_backward()` to show you what we mean and you can fill in the specifics.  Test your function by defining values to represent `x` and `y` and then computing `z = x*y`.  Kickoff the base case by setting `z.grad = 1`, then call `_backward` on the x, y, and z in an appropriate order (think about what order these have to be called in).  Finally, make sure the values of `x.grad`, `y.grad`, and `z.grad` are as expected by visualize your dataflow diagram using `draw_dot`.\n",
        "\n",
        "**Important question to ponder:** In the template code, why did we define `_backward` using `+=` instead of `=`?"
      ],
      "metadata": {
        "id": "VfPH-M5ETnz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_dot(root, format='svg', rankdir='LR'):\n",
        "    \"\"\"\n",
        "    format: png | svg | ...\n",
        "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
        "    \"\"\"\n",
        "    assert rankdir in ['LR', 'TB']\n",
        "    nodes, edges = trace(root)\n",
        "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
        "\n",
        "    for n in nodes:\n",
        "        dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')\n",
        "        if n.op:\n",
        "            dot.node(name=str(id(n)) + n.op, label=n.op)\n",
        "            dot.edge(str(id(n)) + n.op, str(id(n)))\n",
        "\n",
        "    for n1, n2 in edges:\n",
        "        dot.edge(str(id(n1)), str(id(n2)) + n2.op)\n",
        "\n",
        "    return dot\n",
        "\n",
        "class Value:\n",
        "    def __init__(self, data, inputs=(), op=\"\"):\n",
        "        self.data = data\n",
        "        self.inputs = set(inputs)\n",
        "        self.op = op\n",
        "        self._backward = lambda : None\n",
        "        # set this to 0, it will be updated by calling `_backward` on various Value objects\n",
        "        self.grad = 0\n",
        "\n",
        "    def __add__(self, other):\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "        # note: this uses a feature of Python called inner functions\n",
        "        # see: https://www.geeksforgeeks.org/python-inner-functions/\n",
        "        def _backward():\n",
        "            # you should assume that out.grad has already been computed\n",
        "            self.grad +=  # fill in this expression!\n",
        "            other.grad +=  # fill in this expression!\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        out = Value(self.data * other.data, (self, other), '+')\n",
        "        def _backward():\n",
        "            # you should assume that out.grad has already been computed\n",
        "            self.grad +=  # fill in this expression!\n",
        "            other.grad +=  # fill in this expression!\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "# fill in some values for x and y, define z, and visualize your graph\n"
      ],
      "metadata": {
        "id": "2kfUDUz0R3ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "He0_h3cjHXvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def draw_dot(root, format='svg', rankdir='LR'):\n",
        "#     \"\"\"\n",
        "#     format: png | svg | ...\n",
        "#     rankdir: TB (top to bottom graph) | LR (left to right)\n",
        "#     \"\"\"\n",
        "#     assert rankdir in ['LR', 'TB']\n",
        "#     nodes, edges = trace(root)\n",
        "#     dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
        "\n",
        "#     for n in nodes:\n",
        "#         dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')\n",
        "#         if n.op:\n",
        "#             dot.node(name=str(id(n)) + n.op, label=n.op)\n",
        "#             dot.edge(str(id(n)) + n.op, str(id(n)))\n",
        "\n",
        "#     for n1, n2 in edges:\n",
        "#         dot.edge(str(id(n1)), str(id(n2)) + n2.op)\n",
        "\n",
        "#     return dot\n",
        "\n",
        "# class Value:\n",
        "#     def __init__(self, data, inputs=(), op=\"\"):\n",
        "#         self.data = data\n",
        "#         self.inputs = set(inputs)\n",
        "#         self.op = op\n",
        "#         self._backward = lambda : None\n",
        "#         # set this to 0, it will be updated by calling `_backward` on various Value objects\n",
        "#         self.grad = 0\n",
        "\n",
        "#     def __add__(self, other):\n",
        "#         out = Value(self.data + other.data, (self, other), '+')\n",
        "#         # note: this uses a feature of Python called inner functions\n",
        "#         # see: https://www.geeksforgeeks.org/python-inner-functions/\n",
        "#         def _backward():\n",
        "#             self.grad += out.grad * 1\n",
        "#             other.grad += out.grad * 1\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "#     def __mul__(self, other):\n",
        "#         out = Value(self.data * other.data, (self, other), '+')\n",
        "#         def _backward():\n",
        "#             self.grad +=  out.grad * other.data\n",
        "#             other.grad +=  out.grad * self.data\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "# x = Value(7)\n",
        "# y = Value(4)\n",
        "# z = x*y\n",
        "# z.grad = 1\n",
        "# z._backward()\n",
        "# x._backward()\n",
        "# y._backward()\n",
        "# draw_dot(z)"
      ],
      "metadata": {
        "id": "7UIqK6bAHXC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 7\n",
        "\n",
        "Add support to your `Value` class for the sigmoid function and log loss functions (log loss should take `self` and `y` as inputs where `y` is either 0 or 1 and represents the true class).  Come up with a suitable test case for your new functions and make sure that your implementation agrees with what you expect.  Make sure that the output and `grad` are computed appropriately (you'll have to call `_backward` in a suitable order).  Note: you should choose something pretty simple here. Avoid doing more than one operation in a single assignment as that will make it hard to properly call `_backward()` (i.e., don't do ``z =(x*y+z).sigmoid()`).  You can use `draw_dot` to verify your answer with some hand-done calculations."
      ],
      "metadata": {
        "id": "s1xIzNLpIWK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "PLSGHVVGcSGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from math import exp, log\n",
        "\n",
        "# class Value:\n",
        "#     def __init__(self, data, inputs=(), op=\"\"):\n",
        "#         self.data = data\n",
        "#         self.inputs = set(inputs)\n",
        "#         self.op = op\n",
        "#         self._backward = lambda : None\n",
        "#         self.grad = 0\n",
        "\n",
        "#     def __add__(self, other):\n",
        "#         out = Value(self.data + other.data, (self, other), '+')\n",
        "#         def _backward():\n",
        "#             self.grad += 1 * out.grad\n",
        "#             other.grad += 1 * out.grad\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "#     def __mul__(self, other):\n",
        "#         out = Value(self.data * other.data, (self, other), '+')\n",
        "#         def _backward():\n",
        "#             self.grad += other.data * out.grad\n",
        "#             other.grad += self.data * out.grad\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "#     def sigmoid(self):\n",
        "#         out = Value(1./(1+exp(-self.data)), (self,), 'sigmoid')\n",
        "#         def _backward():\n",
        "#             self.grad += out.grad * out.data * (1 - out.data)\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "#     def logloss(self, y):\n",
        "#         # Note: self is yhat and y is the actual value\n",
        "#         out = Value(-y.data*log(self.data) - (1-y.data)*log(1-self.data), (self, y), 'logloss')\n",
        "#         def _backward():\n",
        "#             self.grad += out.grad * (-y.data/self.data + (1-y.data)/(1-self.data))\n",
        "#             y.grad += out.grad * (-y.data*log(self.data)+log(1-self.data))\n",
        "\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "# x = Value(3)\n",
        "# z = x.sigmoid()\n",
        "# y = Value(1.0)\n",
        "# log_loss = z.logloss(y)\n",
        "# print(f\"{z.data}\")       # this should be 1/(1+exp(-3)) = 0.9525741268224334\n",
        "# print(f\"{log_loss.data}\")       # this should be -log(0.9525741268224334) = 0.04858735157374191(note: log is natural log)\n",
        "# log_loss.grad = 1\n",
        "# log_loss._backward()\n",
        "# z._backward()\n",
        "# y._backward()\n",
        "# x._backward()\n",
        "# draw_dot(log_loss)"
      ],
      "metadata": {
        "id": "u_CLbqKkUxVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 8\n",
        "\n",
        "Add the `backward` function that we provide to our `Value` class (or if you are super ambitious you can implement it by learning about the concept of [topological sorting](https://en.wikipedia.org/wiki/Topological_sorting), which will automatically ensure that the `_backward` functions are called in the correct order).  In your response, explain what would could happen if the `_backward` functions are not called in a suitable order.\n",
        "\n",
        "Use the `backward` function to evaluate the partial derivatives of $f = xy + z^2x$ with respect to $x$, $y$, and $z$.  Instead of manually having to run `_backward` on each `Value` object, you can just run `backward` on `f`."
      ],
      "metadata": {
        "id": "W6T3-tciI6Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add this to your `Value` class (make sure to fix the indentation)\n",
        "def backward(self):\n",
        "    # set our base case\n",
        "    self.grad = 1\n",
        "    # topological order all of the children in the graph\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "        if v not in visited:\n",
        "            visited.add(v)\n",
        "            for input in v.inputs:\n",
        "                build_topo(input)\n",
        "            topo.append(v)\n",
        "    build_topo(self)\n",
        "\n",
        "    # go one variable at a time and apply the chain rule to get its gradient\n",
        "    self.grad = 1\n",
        "    for v in reversed(topo):\n",
        "        v._backward()\n",
        "\n",
        "# create the specified expression and test that the gradients are correct."
      ],
      "metadata": {
        "id": "PkQsNdQ5KfKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "yAhPtGAOKuKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Value:\n",
        "#     def __init__(self, data, inputs=(), op=\"\"):\n",
        "#         self.data = data\n",
        "#         self.inputs = set(inputs)\n",
        "#         self.op = op\n",
        "#         self._backward = lambda : None\n",
        "#         self.grad = 0\n",
        "\n",
        "#     def __add__(self, other):\n",
        "#         out = Value(self.data + other.data, (self, other), '+')\n",
        "#         def _backward():\n",
        "#             self.grad += 1 * out.grad\n",
        "#             other.grad += 1 * out.grad\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "#     def __mul__(self, other):\n",
        "#         out = Value(self.data * other.data, (self, other), '+')\n",
        "#         def _backward():\n",
        "#             self.grad += other.data * out.grad\n",
        "#             other.grad += self.data * out.grad\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "#     def sigmoid(self):\n",
        "#         out = Value(1./(1+exp(-self.data)), (self,), 'sigmoid')\n",
        "#         def _backward():\n",
        "#             self.grad += out.grad * out.data * (1 - out.data)\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "#     def logloss(self, y):\n",
        "#         # Note: self is yhat and y is the actual value\n",
        "#         out = Value(-y.data*log(self.data) - (1-y.data)*log(1-self.data), (self, y), 'logloss')\n",
        "#         def _backward():\n",
        "#             self.grad += out.grad * (-y.data/self.data + (1-y.data)/(1-self.data))\n",
        "#             y.grad += out.grad * (-y.data*log(self.data)+log(1-self.data))\n",
        "\n",
        "#         out._backward = _backward\n",
        "#         return out\n",
        "\n",
        "#     def backward(self):\n",
        "#         # set our base case\n",
        "#         self.grad = 1\n",
        "#         # topological order all of the children in the graph\n",
        "#         topo = []\n",
        "#         visited = set()\n",
        "#         def build_topo(v):\n",
        "#             if v not in visited:\n",
        "#                 visited.add(v)\n",
        "#                 for input in v.inputs:\n",
        "#                     build_topo(input)\n",
        "#                 topo.append(v)\n",
        "#         build_topo(self)\n",
        "\n",
        "#         # go one variable at a time and apply the chain rule to get its gradient\n",
        "#         self.grad = 1\n",
        "#         for v in reversed(topo):\n",
        "#             v._backward()\n",
        "\n",
        "# x = Value(3)\n",
        "# y = Value(8)\n",
        "# z = Value(2)\n",
        "# f = x*y + z*z*x\n",
        "# f.backward()\n",
        "# print(f\"df_dx {x.grad}\")        # should be 12\n",
        "# print(f\"df_dy {y.grad}\")        # should be 3\n",
        "# print(f\"df_dz {z.grad}\")        # should be 12"
      ],
      "metadata": {
        "id": "GCODYDr7bwqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 9\n",
        "\n",
        "Now, it's all going to come together!  In this exercise you will use your `Value` class to perform machine learning on the [Wisconsin Breast Cancern dataset](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic), which is a classic dataset in machine learning.  In this dataset, you will use machine learning for medical diagnosis (in this case determining if a tumor is malignant or benign from a set of features derived from images of a tumor).\n",
        "\n",
        "We have left several TODOs in the starter code below.  The high-level steps are listed below, and we have specified whether we have implemented these for you or you should implement them yourself.\n",
        "\n",
        "1.  Load the data, scale the features, create a train / test split, and define a helper function for determining accuracy of the logistic regression on the test set (we provide this)\n",
        "2.  Initialize the weights and bias term of the logistic regression to 0.0.\n",
        "3.  Compute the total log loss of the model on the training set (you will do this).\n",
        "4.  Call backward to populate the `.grad` (you will do this)\n",
        "5.  Adjust the weights and bias using the computed gradient (you will do this).\n",
        "6.  Reset the gradients for the next step (we provide this)\n",
        "7.  Compute the accuracy of the model on the test set (we provide this)"
      ],
      "metadata": {
        "id": "kJ1fYJgKflbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from math import exp\n",
        "\n",
        "def get_test_accuracy(X_test, y_test, weights, bias):\n",
        "    # compute the accuracy on the test set\n",
        "    correct = 0\n",
        "    for i in range(X_test.shape[0]):\n",
        "        s = bias.data\n",
        "        for j in range(X_train.shape[1]):\n",
        "            s = s + weights[j].data * X_test[i,j]\n",
        "        p = 1./(1+exp(-s))\n",
        "        if p > 0.5:\n",
        "            y_pred = 1\n",
        "        else:\n",
        "            y_pred = 0\n",
        "        if y_pred == y_test[i]:\n",
        "            correct += 1\n",
        "    return correct\n",
        "\n",
        "# see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=3)\n",
        "s = StandardScaler()\n",
        "s.fit(X_train)\n",
        "X_train = s.transform(X_train)\n",
        "X_test = s.transform(X_test)\n",
        "print(f\"X_train.shape {X_train.shape} y_train.shape {y_train.shape} X_test.shape {X_test.shape}\")\n",
        "print(f\"feature names={data.feature_names}\")\n",
        "# this defines the number of times to loop through the dataset\n",
        "n_epochs = 10\n",
        "\n",
        "# create a weight for each feature and set it to 0\n",
        "weights = [Value(0.0) for _ in range(len(data.feature_names))]\n",
        "# this is a constant value (like the y-intercept in linear regression)\n",
        "bias = Value(0.0)\n",
        "\n",
        "lam = 10**-2\n",
        "correct = get_test_accuracy(X_test, y_test, weights, bias)\n",
        "print(f\"correct / total ({correct} / {X_test.shape[0]})\")\n",
        "for epoch in range(n_epochs):\n",
        "    print(f\"epoch {epoch}\")\n",
        "    total_loss = Value(0.0)\n",
        "    # TODO: compute the total loss over the training set using your `Value` class\n",
        "    # Hint: you'll want to look up how to loop over the rows and columns of a numpy array\n",
        "    pass\n",
        "\n",
        "    # TODO: call `.backward()` on total_loss to populate the `.grad` varaiables\n",
        "    pass\n",
        "\n",
        "    # TODO: adjust the weights and bias term by performing a step of gradient descent\n",
        "    # using the provide step size `lam`\n",
        "    pass\n",
        "\n",
        "    # reset the grad variables for the next step\n",
        "    for j in range(X_train.shape[1]):\n",
        "        weights[j].grad = 0.0\n",
        "    bias.brad = 0.0\n",
        "\n",
        "    # print out the accuracy\n",
        "    correct = get_test_accuracy(X_test, y_test, weights, bias)\n",
        "    print(f\"correct / total ({correct} / {X_test.shape[0]})\")"
      ],
      "metadata": {
        "id": "Zm_2K39uvuz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "BF8fNwZ4UhZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.datasets import load_breast_cancer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from math import exp\n",
        "\n",
        "# def get_test_accuracy(X_test, y_test, weights, bias):\n",
        "#     # compute the accuracy on the test set\n",
        "#     correct = 0\n",
        "#     for i in range(X_test.shape[0]):\n",
        "#         s = bias.data\n",
        "#         for j in range(X_train.shape[1]):\n",
        "#             s = s + weights[j].data * X_test[i,j]\n",
        "#         p = 1./(1+exp(-s))\n",
        "#         if p > 0.5:\n",
        "#             y_pred = 1\n",
        "#         else:\n",
        "#             y_pred = 0\n",
        "#         if y_pred == y_test[i]:\n",
        "#             correct += 1\n",
        "#     return correct\n",
        "\n",
        "# # see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
        "# data = load_breast_cancer()\n",
        "# X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=3)\n",
        "# s = StandardScaler()\n",
        "# s.fit(X_train)\n",
        "# X_train = s.transform(X_train)\n",
        "# X_test = s.transform(X_test)\n",
        "# print(f\"X_train.shape {X_train.shape} y_train.shape {y_train.shape} X_test.shape {X_test.shape}\")\n",
        "# print(f\"feature names={data.feature_names}\")\n",
        "# # this defines the number of times to loop through the dataset\n",
        "# n_epochs = 10\n",
        "\n",
        "# # create a weight for each feature and set it to 0\n",
        "# weights = [Value(0.0) for _ in range(len(data.feature_names))]\n",
        "# # this is a constant value (like the y-intercept in linear regression)\n",
        "# bias = Value(0.0)\n",
        "\n",
        "# lam = 10**-2\n",
        "# correct = get_test_accuracy(X_test, y_test, weights, bias)\n",
        "# print(f\"correct / total ({correct} / {X_test.shape[0]})\")\n",
        "# for epoch in range(n_epochs):\n",
        "#     print(f\"epoch {epoch}\")\n",
        "#     # compute the accuracy on the test set\n",
        "#     correct = 0\n",
        "#     for i in range(X_test.shape[0]):\n",
        "#         s = bias.data\n",
        "#         for j in range(X_train.shape[1]):\n",
        "#             s = s + weights[j].data * X_test[i,j]\n",
        "#         p = 1./(1+exp(-s))\n",
        "#         if p > 0.5:\n",
        "#             y_pred = 1\n",
        "#         else:\n",
        "#             y_pred = 0\n",
        "#         if y_pred == y_test[i]:\n",
        "#             correct += 1\n",
        "#     total_loss = Value(0.0)\n",
        "#     # TODO: compute the total loss over the training set using your `Value` class\n",
        "#     for i in range(X_train.shape[0]):\n",
        "#         s = bias\n",
        "#         for j in range(X_train.shape[1]):\n",
        "#             s = s + weights[j] * Value(X_train[i,j])\n",
        "#         p = s.sigmoid()\n",
        "#         loss = p.logloss(Value(y_train[i]))\n",
        "#         total_loss = total_loss + loss\n",
        "\n",
        "#     # TODO: call `.backward()` on total_loss\n",
        "#     total_loss.backward()\n",
        "#     # TODO: adjust the weights and bias term by performing a step of gradient descent\n",
        "#     # using the provide step size `lam`\n",
        "#     for j in range(X_train.shape[1]):\n",
        "#         weights[j].data -= lam * weights[j].grad\n",
        "#     bias.data -= lam * bias.grad\n",
        "\n",
        "#     # reset the grad variables for the next step\n",
        "#     for j in range(X_train.shape[1]):\n",
        "#         weights[j].grad = 0.0\n",
        "#     bias.brad = 0.0\n",
        "#     correct = get_test_accuracy(X_test, y_test, weights, bias)\n",
        "#     print(f\"correct / total ({correct} / {X_test.shape[0]})\")"
      ],
      "metadata": {
        "id": "7HO3YPUTR4dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 10\n",
        "\n",
        "What just happened?  Summarize in your own words what we did in this assignment.  How might the framework we developed help us build more complex machine learning models (e.g., like the ones show in the 3B1B video [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk)?"
      ],
      "metadata": {
        "id": "mEZgM4J-fGDD"
      }
    }
  ]
}