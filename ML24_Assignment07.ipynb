{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxJe2v6FSSrMjNG77us7xb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olinml2024/notebooks/blob/main/ML24_Assignment07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd\n",
        "\n",
        "**High-level picture:** In this assignment you'll be implementing your very own system for automatic differentiation.  You'll then see how this framework can be used for solving a range of machine learning problems by using your autograd engine within a gradient descent optimizer.  We'll conclude by comparing the system your autograd engine with the popular machine learning library `pytorch`.\n",
        "\n",
        "Credits: the design of the autograd engine you will be implementing is based on the micrograd framework from Andrej Karpathy.  You may find it useful [to watch Andrej Karpathy's walkthrough video](https://www.youtube.com/watch?v=VMj-3S1tku0&t=2414s) of this.  A lot of what we've done in the first 2/3 of the assignment follows along the same lines, but we felt it important to explicitly define exercises so that you are actively doing things and reasoning about what you are implementing rather than just passively watching."
      ],
      "metadata": {
        "id": "W4XJxHcvIzCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Recall the definition of a derivative and a partial derivative and how it can be approximated numerically.  Test on a simple function and make sure the calculus and the numerical approximation agree.\n",
        "\n",
        "2. Create a `Value` class that provides represents a scalar value.  Define an addition function that allows you to add Value objects together.  Keep a placeholder to store any Values that feed into the returned value.\n",
        "\n",
        "3. Add a multiplication function to your class.\n",
        "\n",
        "4. Draw a dataflow diagram representing a particular expression that we give (involving additions and multiplications).  Work out the partial derivatives.\n",
        "\n",
        "5. Add the provided `draw_dot` function and use it to draw the dataflow diagram from 4.\n",
        "\n",
        "6. Add a placeholder to store a partial derivative with our value function.  It will represent the partial derivative of the final expression with respect to the value.  We'll give you a template for the `_backward()` function and you will fill in the blanks for addition and multiplication.  This should match what we did in class on day 6.\n",
        "\n",
        "7. Add support for the sigmoid function and log loss.\n",
        "\n",
        "8. Add the backward function that we provide (or implement it if you want).  We'll explain what's happening.  Students will explain why the reverse topological sort is necessary.\n",
        "\n",
        "9. Implement a logistic regression model on the Titanic dataset.  Use a fixed learning rate.  We'll provide a reference implementation in Pytorch.  Make sure they converge to the same thing.\n",
        "\n",
        "10. (bonus) experiment with different learning rates or learning rate schedules to see how it changes the speed at which the model converges."
      ],
      "metadata": {
        "id": "8I_k_hR3KkYA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-si4gdfI3GU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}