{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPQ/dUwb8Li3W0QB2uNn/Oh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olinml2024/notebooks/blob/main/ML24_Day13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivating Word Embeddings\n",
        "\n",
        "## Toy Problem\n",
        "\n",
        "We've done a mini-lecture to explain some key ideas behind the notion of word embeddings.  Now we're going to actually see some in action.  In this notebook, we are going to start with a toy problem to help us wrap our minds around the key ideas.\n",
        "\n",
        "We'll begin by embedding not words but single letters.  We decided to start with this because we wanted to make our first example as simple as possible.  First, we will create a dataset consisting of 10,000 letter patterns.  Each letter pattern will either consist of two vowels, a consonant, and then two more vowels (VVCVV) or two consontants, a vowel, and then two more consontants (CCVCC).\n",
        "\n",
        "After generating these patterns, we'll print some out so you can see what they look like."
      ],
      "metadata": {
        "id": "TFMpHCW0auEF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKIihGbEZoQf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# patterns are VVCVV CCVCC\n",
        "n_train = 10000\n",
        "vowels = set('aeiou')\n",
        "consonants = set('bcdfghjklmnpqrstvwxyz')\n",
        "\n",
        "def make_patterns(n):\n",
        "    X = []\n",
        "    for i in range(n):\n",
        "        if random.randint(0, 1) == 1:\n",
        "            random_vowel = random.choice(tuple(vowels))\n",
        "            four_random_consonants = np.random.choice(tuple(consonants), 4)\n",
        "            X.append([four_random_consonants[0], four_random_consonants[1], random_vowel, four_random_consonants[2], four_random_consonants[3]])\n",
        "        else:\n",
        "            random_consonant = random.choice(tuple(consonants))\n",
        "            four_random_vowels = np.random.choice(tuple(vowels), 4)\n",
        "            X.append([four_random_vowels[0], four_random_vowels[1], random_consonant, four_random_vowels[2], four_random_vowels[3]])\n",
        "    return X\n",
        "\n",
        "X = make_patterns(n_train)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, were going to train a character embedding.  We begin by defining two dictionaries.  The first, called ``tokenize`` is for turning each of the 26 letters in to a token (represented as an integer from 0-25).  The second, called ``detokenize``, implements the inverse operation (from integer to letter)."
      ],
      "metadata": {
        "id": "huqg4WNbboQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize = dict(zip(list(vowels) + list(consonants), list(range(26))))\n",
        "detokenize = dict(zip(tokenize.values(), tokenize.keys()))\n",
        "\n",
        "print(tokenize['a'])                    # should be 0\n",
        "print(detokenize[tokenize['a']])        # should get back to 'a'"
      ],
      "metadata": {
        "id": "R1YIGDegZtSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can define what's called the skip-gram model.  The skip-gram model, in this case, attempts to use the center letter to predict the surrounding letters.  For example, if we have a CCVCC pattern, we would want to be able to take the center letter (vowel) as input and predict the identity of the surrounding consonants.\n",
        "\n",
        "You may be thinking this is an impossible task.  You'd be right.  Let's take a minute to discuss this.\n",
        "\n",
        "Nevertheless, we can define a model that does its best to solve the task.  Let's take a look at this model in detail."
      ],
      "metadata": {
        "id": "yFdmcov6cNWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class SkipGram(Module):\n",
        "    def __init__(self, n_embd=2):\n",
        "        \"\"\"\n",
        "        Create the SkipGram model.\n",
        "        Args:\n",
        "          n_embd: the number of dimensions to use for our character embeddings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # hardcoding 26 (note: could also use len(tokenize))\n",
        "        self.embedding = nn.Embedding(26, n_embd)\n",
        "        self.output = nn.Linear(n_embd, 26)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        The forward pass of our model\n",
        "        Args:\n",
        "          x: the center letter (represented as an integer from 0-25)\n",
        "\n",
        "        Returns: the log probability of each of the 26 letters.\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)\n",
        "        x = self.output(x)\n",
        "        x = nn.functional.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "model = SkipGram()\n",
        "\n",
        "# start off by tokenize each letter in each of our 10,000 patterns\n",
        "X_tok = torch.tensor([[tokenize[letter] for letter in x] for x in X])\n",
        "# we are predicting the first two and last two characters\n",
        "y_tok = torch.hstack((X_tok[:,0:2], X_tok[:,3:5]))\n",
        "X_tok = X_tok[:,2]\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=10**-2)\n",
        "n_epochs = 500\n",
        "losses = np.zeros((n_epochs,1))\n",
        "for n in range(n_epochs):\n",
        "    log_probs = model(X_tok)\n",
        "    # this is fancy syntax grabs the corresponding log probabilities corresponding\n",
        "    # to the letters that actually surrounded our center letter.\n",
        "    loss = -log_probs.gather(1,y_tok).mean()\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    losses[n] = loss.item()\n",
        "    optimizer.step()\n",
        "\n",
        "plt.plot(range(n_epochs), losses)\n",
        "plt.ylabel('negative log likelihood')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vq1u_eh_ZyHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can look at the embeddings themselves as a way to understand what the model learned."
      ],
      "metadata": {
        "id": "7lA1m-KAeB8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "embedded = model.embedding(torch.tensor(list(tokenize.values()))).detach().numpy()\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(embedded[:,0], embedded[:,1])\n",
        "text = list(tokenize.keys())\n",
        "for i in range(embedded.shape[0]):\n",
        "    ax.annotate(text[i], (embedded[i,0], embedded[i,1]))"
      ],
      "metadata": {
        "id": "qV7yyGtlZ3Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip-grams on Words\n",
        "\n",
        "We can extend these same ideas to a little bit more interesting problem.  We begin with an old dataset of text called [the Penn Tree Bank](https://catalog.ldc.upenn.edu/LDC99T42).\n",
        "\n",
        "You can use the link above for a more complete description, but a very high-level overview is that the dataset contains \"one million words of 1989 Wall Street Journal material.\"  This dataset is convenient in that the sample we will load has already been tokenized (case removed, uncommon words replaced by a special symbol, etc.).\n",
        "\n",
        "We'll repeat largely the same steps as we did for our letter example but we'll use word as a our unit of analysis. We'll also build in support for using the GPU, batches of data, and allow the context window size to be adjusted.\n"
      ],
      "metadata": {
        "id": "PNBVH8bg-v1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tomsercu/lstm/refs/heads/master/data/ptb.train.txt -O ptb.train.txt\n",
        "!wget https://raw.githubusercontent.com/tomsercu/lstm/refs/heads/master/data/ptb.test.txt -O ptb.test.txt\n",
        "with open('ptb.train.txt') as f:\n",
        "    text = f.read()\n",
        "words = text.split()\n",
        "unique_words = set(words)"
      ],
      "metadata": {
        "id": "8AKDeu5hajxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_words = dict(zip(list(unique_words), list(range(len(unique_words)))))\n",
        "detokenize_words = dict(zip(tokenize_words.values(), tokenize_words.keys()))\n",
        "\n",
        "class SkipGram_words(Module):\n",
        "    def __init__(self, n_embd=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(unique_words), n_embd)\n",
        "        self.linear_1 = nn.Linear(n_embd, len(unique_words))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.linear_1(x)\n",
        "        x = nn.functional.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "model = SkipGram_words(n_embd=100)\n",
        "# use a bigger window size (this seemed to give interesting results)\n",
        "sliding_window_size = 9\n",
        "assert(sliding_window_size % 2 == 1)\n",
        "center_idx = (sliding_window_size-1)//2\n",
        "\n",
        "all_contexts = []\n",
        "# use sliding window\n",
        "for i in range(0, len(words)-sliding_window_size+1):\n",
        "    all_contexts.append([tokenize_words[w] for w in words[i:i+sliding_window_size]])\n",
        "\n",
        "X_tok = torch.tensor(all_contexts)\n",
        "y_tok = torch.hstack((X_tok[:,:center_idx], X_tok[:,center_idx+1:]))\n",
        "X_tok = X_tok[:,center_idx]\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=10**-3, weight_decay=10**-4)\n",
        "\n",
        "trainset = torch.utils.data.TensorDataset(X_tok, y_tok)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "n_epochs = 10\n",
        "for n in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    total_processed = 0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        log_probs = model(inputs.to(device))\n",
        "        loss = -log_probs.gather(1,labels.to(device)).mean()\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        total_processed += inputs.shape[0]\n",
        "        if i % 200 == 0:\n",
        "            print(total_processed/len(all_contexts))\n",
        "            print(f\"{n} {running_loss/(i+1)}\")"
      ],
      "metadata": {
        "id": "qW88erGgap3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the embeddings of the 500 most common words and look for the most similar word for each."
      ],
      "metadata": {
        "id": "t41sYcFNfzV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "from collections import Counter\n",
        "\n",
        "c = Counter(words)\n",
        "most_common_words, most_common_word_inds = \\\n",
        "    zip(*map(lambda k: (k[0], tokenize_words[k[0]]),\n",
        "         c.most_common(500)))\n",
        "most_common_word_inds_as_tensor = torch.tensor(most_common_word_inds).to(device)\n",
        "common_word_embeddings = model.embedding(most_common_word_inds_as_tensor).detach().cpu().numpy()\n",
        "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(common_word_embeddings)\n",
        "distances, indices = nbrs.kneighbors(common_word_embeddings)\n",
        "closest_map = dict(zip(most_common_words, [most_common_words[ind] for ind in indices[:,1]]))"
      ],
      "metadata": {
        "id": "kNafGR0tbl2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closest_map"
      ],
      "metadata": {
        "id": "4XIaLhvFb0C3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}