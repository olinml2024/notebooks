{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CoXDlTaD43zh",
        "fNMlvQGo7yNy",
        "db_34iuz_tk5"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPYYnXCfp1AXLuzSKD5PBHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olinml2024/notebooks/blob/main/ML24_Assignment10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis of Movie Reviews\n",
        "\n",
        "In this notebook, you'll be working with the [Stanford Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "Here is the most important information from the README file.  Note that there is more information in the README (e.g., on how the files in the dataset are laid out), but this won't be particularly important to us.\n",
        "\n",
        "# Large Movie Review Dataset v1.0\n",
        "\n",
        "## Overview\n",
        "\n",
        "This dataset contains movie reviews along with their associated binary\n",
        "sentiment polarity labels. It is intended to serve as a benchmark for\n",
        "sentiment classification. This document outlines how the dataset was\n",
        "gathered, and how to use the files provided.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. The overall distribution of labels is balanced (25k pos and 25k neg). We also include an additional 50,000 unlabeled documents for unsupervised learning.\n",
        "\n",
        "In the entire collection, no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings. Further, the train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels.  In the labeled train/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10. Thus reviews with more neutral ratings are not included in the train/test sets. In the unsupervised set, reviews of any rating are included and there are an even number of reviews > 5 and <= 5."
      ],
      "metadata": {
        "id": "qYvY8MxH9wW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start out by downloading the dataset, extracting it, and parsing it."
      ],
      "metadata": {
        "id": "yeiB0qX02W1L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30Xdfb-A8lhl"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import glob\n",
        "\n",
        "gdown.download(id='1AhSDUrPbRPS0JorkDR8W1ZoyeFs8vF55', output='aclImdb_v1.tar.gz')\n",
        "!tar -xvzf aclImdb_v1.tar.gz > /dev/null 2>&1\n",
        "\n",
        "positive_reviews = []\n",
        "\n",
        "# Note: there is a test set we can use as well\n",
        "for file in glob.glob('aclImdb/train/pos/*'):\n",
        "    with open(file) as f:\n",
        "        positive_reviews.append(f.read())\n",
        "\n",
        "negative_reviews = []\n",
        "\n",
        "for file in glob.glob('aclImdb/train/neg/*'):\n",
        "    with open(file) as f:\n",
        "        negative_reviews.append(f.read())\n",
        "\n",
        "print(f\"number of positive reviews {len(positive_reviews)}\")\n",
        "print(f\"number of negative reviews {len(negative_reviews)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we do some machine learning, let's look at some of the reviews.  First, some positive reviews."
      ],
      "metadata": {
        "id": "ItdJW75y-fkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import textwrap\n",
        "\n",
        "for review in random.choices(positive_reviews, k=5):\n",
        "    print(\"POSITIVE REVIEW:\")\n",
        "    print('\\n'.join(textwrap.wrap(review, 50, break_long_words=False)))"
      ],
      "metadata": {
        "id": "N0gOM7UU-hUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now some of the negative reviews."
      ],
      "metadata": {
        "id": "aGcBDIBA_2j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for review in random.choices(negative_reviews, k=5):\n",
        "    print(\"NEGATIVE REVIEW:\")\n",
        "    print('\\n'.join(textwrap.wrap(review, 50, break_long_words=False)))"
      ],
      "metadata": {
        "id": "4YBEwDo-_41w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Our Bag of Words\n",
        "\n",
        "While we're not going to have you create your own implementation, it might be helpful to think through the major steps you would go through to compute a bag of words representation for each of the reviews in this dataset."
      ],
      "metadata": {
        "id": "dSb742OVprNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## *Notebook Exercise 1*\n",
        "\n",
        "Given the reviews stored in positive_reviews and negative_reviews, what would be the major steps you'd need to do in order to compute a bag of words representation?  You can make this as high-level as it needs to be, e.g., you may know that you need to convert the reviews into individual words, but you may not have much of an idea how to do that.  That's alright!  Having a high-level picture is much more important than getting down into the minutiae."
      ],
      "metadata": {
        "id": "hR7tDYKa4trr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expand for Solution"
      ],
      "metadata": {
        "id": "CoXDlTaD43zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roughly you'd need to do the following steps.\n",
        "1. Iterate over all the reviews and convert the reviews into lists of words.  This would require word tokenization and we'd have to think about how we treat non-letters, letter case, etc.\n",
        "2. Assign a unique dimension in our bag of words vector for each word in our dataset.  A dictionary could be handy for storing a mapping from words to vector dimensions.\n",
        "3. Allocate a tensor to store our dataset (this would have a number of rows equal to the number of data points and columns equal to the number of unique words).\n",
        "4. Iterate through each reviews words and increment the appropriate entry in our tensor to \"count\" the words."
      ],
      "metadata": {
        "id": "ZdaLadfI5v-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sklearn's Vectorizer\n",
        "\n",
        "It's certainly not a bad use of time to implement your own bag of words, but we recommend using an off-the-shelf implementation.  Scikit learn has a really nice implementation that can generate a bag of words representation fairly easily.  In the code below, we create the bag of words representation, make a train test split, and print out the bag of words representation as a dictionary for a specific review.  Remember, though, that the bag of words representation converts text into a sparse vector (the dictionary is just to make it easier for us to parse as humans)."
      ],
      "metadata": {
        "id": "xkpsjmcs48bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_reviews = positive_reviews + negative_reviews\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(all_reviews)\n",
        "y = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "row_ind = 10000\n",
        "bow_as_dict = dict(zip(vectorizer.get_feature_names_out()[X.getrow(row_ind).indices].tolist(),\n",
        "                       X.getrow(row_ind).data.tolist())\n",
        "                  )\n",
        "print(bow_as_dict)"
      ],
      "metadata": {
        "id": "aM30-9Mgpt7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Model to Predict Movie Review Sentiment\n",
        "\n",
        "Thinking back to the big picture of this assignment, we are trying to see how we can use the techniques from the last module (learning as optimization) to process text.  We are basically ready to take the final step and try one of our machine learning models on the task of classifying movie review sentiment.\n",
        "\n",
        "Before we do so though, we need to convert our data to pytorch.  This will be a little bit more involved than normal since the ``CountVectorizer`` returns data in a sparse format (this reduces memory and speeds up our computations).  We've provided you with some code to go from scipy sparse matrices (what is returned from ``CountVectorizer``) to a pytorch sparse tensor.\n",
        "\n",
        "We'll also move the data to the GPU (if you selected a GPU-based runtime in Colab)."
      ],
      "metadata": {
        "id": "BB8dnrHL52Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from scipy.sparse import coo_matrix\n",
        "import numpy as np\n",
        "\n",
        "def scipy_to_tensor(X):\n",
        "    coo = coo_matrix(X)\n",
        "\n",
        "    values = coo.data\n",
        "    indices = np.vstack((coo.row, coo.col))\n",
        "\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = coo.shape\n",
        "\n",
        "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "y_train_torch = torch.tensor(y_train).unsqueeze(dim=1).float().to(device)\n",
        "y_test_torch = torch.tensor(y_test).unsqueeze(dim=1).float().to(device)\n",
        "X_train_torch = scipy_to_tensor(X_train).to(device)\n",
        "X_test_torch = scipy_to_tensor(X_test).to(device)"
      ],
      "metadata": {
        "id": "5QcY0Tj9p1We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Exercise 2\n",
        "\n",
        "Using what you just did in the small data project, use pytorch to train a model on the movie review data.  We will leave the particulars up to you, but we'd probably start with logistic regression as it is the easiest baseline to start with (we'll show some interesting ways to analyze such a model in our solution as well).\n",
        "\n",
        "Here is some advice, although please post any issues that come up to Slack.\n",
        "1. Don't use DataLoader / batches (we had trouble making this work with sparse matrices).\n",
        "2. If you want to measure accuracy, you may find the `binary_accuracy` function in the `torcheval` module useful (see our solutions for an example of that).\n",
        "3. As we did before, try to generate a learning curve for training and test set (loss) as well as the accuracy of, at a minimum, your model on your test set."
      ],
      "metadata": {
        "id": "xTSpTbPLr0Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expand for Solution"
      ],
      "metadata": {
        "id": "fNMlvQGo7yNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval\n",
        "from torcheval.metrics.functional import binary_accuracy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        # since we just have a binary classifier, we can get by with just 1\n",
        "        # output dimension this makes computing the accuracy a bit more involved\n",
        "        # than using argmax, which we solve using torcheval.\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "model = LogisticRegression(X_train_torch.shape[1]).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# You can use SGD, but Adam converges quite fast.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "n_epochs = 50\n",
        "train_losses = np.zeros((n_epochs,))\n",
        "test_losses = np.zeros((n_epochs,))\n",
        "test_accuracies = np.zeros((n_epochs,))\n",
        "\n",
        "for n in range(n_epochs):\n",
        "    # this is a good habit to get into (setting the model's mode to train). It\n",
        "    # won't change anything for this model, but some model layers behave\n",
        "    # differently when training versus test.\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train_torch)\n",
        "    loss = criterion(output, y_train_torch)\n",
        "    train_losses[n] = loss.item()\n",
        "    # this is like the ridge term in ridge regression.  It makes the weights\n",
        "    # more interpretable (see below)\n",
        "    l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "    loss = loss + 0.001 * l2_norm\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # set the model to eval mode to make sure any pytorch modules that\n",
        "        # behave differently for evaluation can adjust.\n",
        "        model.eval()\n",
        "        output = model(X_test_torch)\n",
        "        loss = criterion(output, y_test_torch)\n",
        "        test_losses[n] = loss.item()\n",
        "        test_accuracies[n] = binary_accuracy(output.squeeze(), y_test_torch.squeeze(), threshold=0.0).item()\n",
        "\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(test_losses, label='test')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('cross entropy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(test_accuracies)\n",
        "plt.ylabel('test accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J1_OzS1drRxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fun, we're also going to print out the words that contribute most to positive sentiment and negative sentiment.  We can do this by analyzing the weights directly.  The code is a bit involved, but hopefully our comments will help."
      ],
      "metadata": {
        "id": "toNFQkdP9dNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# we are grabbing the weights.  We need to do a few conversions to get them in\n",
        "# the right format.\n",
        "feature_weights = model.linear.weight.data.cpu().numpy()[0]\n",
        "\n",
        "# np.argsort gives us the indices that sort a list rather than the sorted list\n",
        "# itself.  This is useful if we don't care about the weights but rather the\n",
        "# feature indices.\n",
        "print(\"Predictive of a bad review\")\n",
        "for idx in np.argsort(feature_weights)[:10]:\n",
        "    print(vectorizer.get_feature_names_out()[idx])\n",
        "\n",
        "print(\"Predictive of a good review\")\n",
        "# now we take the indices at the end rather than the beginning (high weights).\n",
        "for idx in np.argsort(feature_weights)[-10:]:\n",
        "    print(vectorizer.get_feature_names_out()[idx])"
      ],
      "metadata": {
        "id": "4sSWEGebsDtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf-idf"
      ],
      "metadata": {
        "id": "lL2Q_sI0-MNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Exercise 3\n",
        "\n",
        "As a last exercise, we want you to learn about the [tf-idf feature](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).  The equations on the page are a bit daunting, but hopefully the ideas make sense.  Having the mental model from the IBM video regarding tf-idf may help (so maybe go watch the end of that video again).  If you find a great resource to learn about tf-idf, please let us know (there are many out there).  Also, bring your questions so we can debrief on this in class on Monday.\n",
        "\n",
        "Adjust the code we gave you to compute the bag of words to instead use sklearn's `TfidfVectorizer` class.  You should just be able to modify the code slightly as shown below.\n",
        "\n",
        "```\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_reviews = positive_reviews + negative_reviews\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(all_reviews)\n",
        "```\n",
        "\n",
        "How does the model performance change when using tf-idf instead of raw counts?  Can you make sense of this pattern."
      ],
      "metadata": {
        "id": "gWGX9ugK_DAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toggle for Solution"
      ],
      "metadata": {
        "id": "db_34iuz_tk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't copy our whole solution code again, but the upshot is the performance doesn't change all that much (it may be a bit worse).  It seems the model is able to effectively handle the raw count data.  Maybe movie reviews are too similar in length for it to matter too much.  It's a bit hard to say without spending a bunch more time investigating.  We'd be excited to dig more into this with you."
      ],
      "metadata": {
        "id": "pCNNFj3J_v_y"
      }
    }
  ]
}